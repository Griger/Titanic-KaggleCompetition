---
title: "Etapa de clasificación"
author: "Gustavo Rivas Gervilla"
date: "18 de abril de 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, tidy = TRUE)
```

```{r}
library(party)
library(xgboost)
```

```{r}
full <- read.csv('data/fullPreproBasicoConTitulodeStephensImputacionEdadStephens.csv')

train <- full[1:891,]
test <- full[892:1309,]
```

```{r}
varsIgnoradas <- c(1,4,9,11,15,17,18)
trainFiltrado = train[,-varsIgnoradas]
set.seed(415)
fit <- cforest(as.factor(Survived) ~ .,
                 data = trainFiltrado, 
                 controls=cforest_unbiased(ntree=2000, mtry=3, trace = TRUE))
```

```{r}
testFiltrado <- test[,-varsIgnoradas]
Prediction <- predict(fit, test, OOB=TRUE, type = "response")
cforestSolution <- data.frame(PassengerID = test$PassengerId, Survived = Prediction)
write.csv(cforestSolution, file = 'sols/cforestPreproBasicoSinChildNiMotherConTituloyEdaddeStephens.csv', row.names = F, quote = F)
```

Cuando no uso las variables Child ni Mother acompañadas del preprocesamiento básico: 0.80861 mejor que con ellas.
Cambiando el preprocesamiento básico para que el agrupamiento de los títulos se haga como lo hace Stephens e ignorando también las variables Child y Mother tenemos: 0.80383 algo peor que con el otro preprocesamiento de título.
Al hacer la imputación de la edad con un rpart como Stephens volvemos a tener la misma puntuación que con el tutorial de Stephens, al fin y al cabo poco estamos cambiando de este tutorial en el preprocesamiento, más que la imputación en el NA del Fare: 0.81340

```{r}
fit <- rpart(Survived ~ ., data=trainFiltrado, method="class")

Prediction <- predict(fit, testFiltrado, type = "class")
rpartSolution <- data.frame(PassengerID = test$PassengerId, Survived = Prediction)
write.csv(rpartSolution, file = 'sols/rpartPreproBasicoSinChildNiMotherConTituloyEdaddeStephens.csv', row.names = F, quote = F)
```

Con rpart y lo mismo que antes como conjuntos obtenemos 0.79426 que era justo lo que obtenía Stephens, claro estamos con el mismo conjunto que él básicamente.

```{r}
predictorsNames = names(trainFiltrado)[names(trainFiltrado) != 'Survived'] #names of columns after data conversion
dtrain<-xgb.DMatrix(data=data.matrix(trainFiltrado[,predictorsNames]),label=trainFiltrado$Survived)
dtest<-data.matrix(testFiltrado[,predictorsNames])

param <- list(  objective           = "binary:logistic", 
                booster             = "gbtree",
                eta                 = 0.001, #0.3 #0.3
                max_depth           = 8, #10
                subsample           = 0.5, #0.7
                colsample_bytree    = 0.6 #0.6
)

nround  = 50 #use 3000 he comentado los parametros anteriores #200
set.seed(2045)
clf <- xgboost(param = param, dtrain, nrounds=nround, min_child_weight = 1, verbose=0)

pred <- predict(clf, dtest)
pred_final <- ifelse(pred>0.5,1,0)

xgbSolution <- data.frame(PassengerID = test$PassengerId, Survived = pred_final)
write.csv(xgbSolution, file = 'sols/xgbPreproBasicoSinChildNiMotherConTituloyEdaddeStephensSIMPLE50.csv', row.names = F, quote = F)
```

Enlace consultado para el codigo anterior: https://www.kaggle.com/varun2145/titanic/xgboost-hypertuned/code Obtengo una puntuacion mucho mas baja  0.71292 probablemente por sobre ajuste.

Al bajar a 30 iteraciones ya bajo el score a 0.80383.

TODO los siguientes experimentos con balanceo de carga y tal seran con estos parametros que empatan con el random forest.

Usando unos parametros mas simples, que parecen obtener un modelo menos complejo, entonces obtengo 0.72249

Simplificando aun mas el numero de iteraciones que se dan en el algoritmo y bajando el eta de 0.3 a 0.001 para que el model sea menos conservativo, TODO creo, obtengo la misma puntuacion que con el tutorial de Trevor 0.81340.



