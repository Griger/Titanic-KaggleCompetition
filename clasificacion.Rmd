---
title: "Etapa de clasificación"
author: "Gustavo Rivas Gervilla"
date: "18 de abril de 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, tidy = TRUE)
```

```{r}
library(party)
library(xgboost)
```

```{r}
full <- read.csv('data/fullPreproBasicoConTitulodeStephensImputacionEdadStephens.csv')

train <- full[1:891,]
test <- full[892:1309,]
```

```{r}
varsIgnoradas <- c(1,4,9,11,15,17,18)
trainFiltrado = train[,-varsIgnoradas]
set.seed(415)
fit <- cforest(as.factor(Survived) ~ .,
                 data = trainFiltrado, 
                 controls=cforest_unbiased(ntree=2000, mtry=3, trace = TRUE))
```

```{r}
testFiltrado <- test[,-varsIgnoradas]
Prediction <- predict(fit, test, OOB=TRUE, type = "response")
cforestSolution <- data.frame(PassengerID = test$PassengerId, Survived = Prediction)
write.csv(cforestSolution, file = 'sols/cforestPreproBasicoSinChildNiMotherConTituloyEdaddeStephens.csv', row.names = F, quote = F)
```

Cuando no uso las variables Child ni Mother acompañadas del preprocesamiento básico: 0.80861 mejor que con ellas.
Cambiando el preprocesamiento básico para que el agrupamiento de los títulos se haga como lo hace Stephens e ignorando también las variables Child y Mother tenemos: 0.80383 algo peor que con el otro preprocesamiento de título.
Al hacer la imputación de la edad con un rpart como Stephens volvemos a tener la misma puntuación que con el tutorial de Stephens, al fin y al cabo poco estamos cambiando de este tutorial en el preprocesamiento, más que la imputación en el NA del Fare: 0.81340

```{r}
fit <- rpart(Survived ~ ., data=trainFiltrado, method="class")

Prediction <- predict(fit, testFiltrado, type = "class")
rpartSolution <- data.frame(PassengerID = test$PassengerId, Survived = Prediction)
write.csv(rpartSolution, file = 'sols/rpartPreproBasicoSinChildNiMotherConTituloyEdaddeStephens.csv', row.names = F, quote = F)
```

Con rpart y lo mismo que antes como conjuntos obtenemos 0.79426 que era justo lo que obtenía Stephens, claro estamos con el mismo conjunto que él básicamente.

```{r}
predictorsNames = names(trainFiltrado)[names(trainFiltrado) != 'Survived'] #names of columns after data conversion
dtrain<-xgb.DMatrix(data=data.matrix(trainFiltrado[,predictorsNames]),label=trainFiltrado$Survived)
dtest<-data.matrix(testFiltrado[,predictorsNames])

param <- list(  objective           = "binary:logistic", 
                booster             = "gbtree",
                eta                 = 0.001, #0.3 #0.3
                max_depth           = 8, #10
                subsample           = 0.5, #0.7
                colsample_bytree    = 0.6 #0.6
)

nround  = 50 #use 3000 he comentado los parametros anteriores #200
set.seed(2045)
clf <- xgboost(param = param, dtrain, nrounds=nround, min_child_weight = 1, verbose=0)

pred <- predict(clf, dtest)
pred_final <- ifelse(pred>0.5,1,0)

xgbSolution <- data.frame(PassengerID = test$PassengerId, Survived = pred_final)
write.csv(xgbSolution, file = 'sols/xgbPreproFijado.csv', row.names = F, quote = F)
```

Enlace consultado para el codigo anterior: https://www.kaggle.com/varun2145/titanic/xgboost-hypertuned/code Obtengo una puntuacion mucho mas baja  0.71292 probablemente por sobre ajuste.

Al bajar a 30 iteraciones ya bajo el score a 0.80383.

TODO los siguientes experimentos con balanceo de carga y tal seran con estos parametros que empatan con el random forest.

Usando unos parametros mas simples, que parecen obtener un modelo menos complejo, entonces obtengo 0.72249

Simplificando aun mas el numero de iteraciones que se dan en el algoritmo y bajando el eta de 0.3 a 0.001 para que el model sea menos conservativo, TODO creo, obtengo la misma puntuacion que con el tutorial de Trevor 0.81340. Y no es justamente el mismo resultado que con Trevor, difieren en 13 predicciones.


```{r}
rpartSol <- read.csv('sols/rpartPreproBasicoSinChildNiMotherConTituloyEdaddeStephens.csv')
randomForestSol <- read.csv('sols/cforestPreproBasicoSinChildNiMotherConTituloyEdaddeStephens.csv')
xgbSol <- read.csv('sols/xgbPreproBasicoSinChildNiMotherConTituloyEdaddeStephensSIMPLE50.csv')

ensambleSum <- rpartSol$Survived + randomForestSol$Survived + xgbSol$Survived

ensambleSol <- ifelse(ensambleSum > 1, 1, 0)
ensambleSol <- data.frame(PassengerID = test$PassengerId, Survived = ensambleSol)
write.csv(ensambleSol, file = 'sols/ensambleBasico.csv', row.names = F, quote = F)
```

Con este ensamble obtenemos peor puntuación que con xgboost o randomForest por separado: 0.80861.

# Clasificación con balanceo de carga

## Undersampling

```{r}
fullDown <- read.csv('data/fullDownPreproFijado.csv')

nTestRows <- 418
nRows <- nrow(fullDown)

train <- fullDown[1 : (nRows - nTestRows), ]
test <- fullDown[(nRows - nTestRows + 1) : nRows,]
```

### cforest

```{r}
varsIgnoradas <- c(1,4,9,11,15,17,18)
trainFiltrado = train[,-varsIgnoradas]
set.seed(415)
fit <- cforest(as.factor(Survived) ~ .,
                 data = trainFiltrado, 
                 controls=cforest_unbiased(ntree=2000, mtry=3, trace = TRUE))
```

```{r}
testFiltrado <- test[,-varsIgnoradas]
Prediction <- predict(fit, test, OOB=TRUE, type = "response")
cforestSolution <- data.frame(PassengerID = test$PassengerId, Survived = Prediction)
write.csv(cforestSolution, file = 'sols/cforestPreproFijadoDownSampling.csv', row.names = F, quote = F)
```

Puntuacion: 0.79426 baja

### rpart

```{r}
fit <- rpart(Survived ~ ., data=trainFiltrado, method="class")

Prediction <- predict(fit, testFiltrado, type = "class")
rpartSolution <- data.frame(PassengerID = test$PassengerId, Survived = Prediction)
write.csv(rpartSolution, file = 'sols/rpartPreproFijadoDownSampling.csv', row.names = F, quote = F)
```
 
Puntuacion: 0.78947 baja

### xgboost

```{r}
predictorsNames = names(trainFiltrado)[names(trainFiltrado) != 'Survived'] #names of columns after data conversion
dtrain<-xgb.DMatrix(data=data.matrix(trainFiltrado[,predictorsNames]),label=trainFiltrado$Survived)
dtest<-data.matrix(testFiltrado[,predictorsNames])

param <- list(  objective           = "binary:logistic", 
                booster             = "gbtree",
                eta                 = 0.001, #0.3 #0.3
                max_depth           = 8, #10
                subsample           = 0.5, #0.7
                colsample_bytree    = 0.6 #0.6
)

nround  = 50 #use 3000 he comentado los parametros anteriores #200
set.seed(2045)
clf <- xgboost(param = param, dtrain, nrounds=nround, min_child_weight = 1, verbose=0)

pred <- predict(clf, dtest)
pred_final <- ifelse(pred>0.5,1,0)

xgbSolution <- data.frame(PassengerID = test$PassengerId, Survived = pred_final)
write.csv(xgbSolution, file = 'sols/xgbPreproFijadoDownSampling.csv', row.names = F, quote = F)
```

Puntuacion: 0.77990 baja mucho todo con respecto al mismo algoritmo con el conjunto sin balancear

## Oversampling

```{r}
fullUp <- read.csv('data/fullUpPreproFijado.csv')

nTestRows <- 418
nRows <- nrow(fullUp)

train <- fullUp[1 : (nRows - nTestRows), ]
test <- fullUp[(nRows - nTestRows + 1) : nRows,]
```

### cforest

```{r}
varsIgnoradas <- c(1,4,9,11,15,17,18)
trainFiltrado = train[,-varsIgnoradas]
set.seed(415)
fit <- cforest(as.factor(Survived) ~ .,
                 data = trainFiltrado, 
                 controls=cforest_unbiased(ntree=2000, mtry=3, trace = TRUE))
```

```{r}
testFiltrado <- test[,-varsIgnoradas]
Prediction <- predict(fit, test, OOB=TRUE, type = "response")
cforestSolution <- data.frame(PassengerID = test$PassengerId, Survived = Prediction)
write.csv(cforestSolution, file = 'sols/cforestPreproFijadoUpSampling.csv', row.names = F, quote = F)
```

Puntuacion: 0.78469 baja y baja con respecto a undersampling (por el solapamiento de clases ??)

### rpart

```{r}
fit <- rpart(Survived ~ ., data=trainFiltrado, method="class")

Prediction <- predict(fit, testFiltrado, type = "class")
rpartSolution <- data.frame(PassengerID = test$PassengerId, Survived = Prediction)
write.csv(rpartSolution, file = 'sols/rpartPreproFijadoUpSampling.csv', row.names = F, quote = F)
```
 
Puntuacion: 0.73684 baja y baja respecto al undersampling

### xgboost

```{r}
predictorsNames = names(trainFiltrado)[names(trainFiltrado) != 'Survived'] #names of columns after data conversion
dtrain<-xgb.DMatrix(data=data.matrix(trainFiltrado[,predictorsNames]),label=trainFiltrado$Survived)
dtest<-data.matrix(testFiltrado[,predictorsNames])

param <- list(  objective           = "binary:logistic", 
                booster             = "gbtree",
                eta                 = 0.001, #0.3 #0.3
                max_depth           = 8, #10
                subsample           = 0.5, #0.7
                colsample_bytree    = 0.6 #0.6
)

nround  = 50 #use 3000 he comentado los parametros anteriores #200
set.seed(2045)
clf <- xgboost(param = param, dtrain, nrounds=nround, min_child_weight = 1, verbose=0)

pred <- predict(clf, dtest)
pred_final <- ifelse(pred>0.5,1,0)

xgbSolution <- data.frame(PassengerID = test$PassengerId, Survived = pred_final)
write.csv(xgbSolution, file = 'sols/xgbPreproFijadoUpSampling.csv', row.names = F, quote = F)
```

Puntuacion: 0.76555 baja y también respecto al undersampling

## Smote

```{r}
fullSMOTE <- read.csv('data/fullSMOTE.csv')

nTestRows <- 418
nRows <- nrow(fullSMOTE)

train <- fullSMOTE[1 : (nRows - nTestRows), ]
test <- fullSMOTE[(nRows - nTestRows + 1) : nRows,]
testPassengerId <- 892:1309
```

### cforest

```{r}
set.seed(415)
fit <- cforest(as.factor(Survived) ~ .,
                 data = train, 
                 controls=cforest_unbiased(ntree=2000, mtry=3, trace = TRUE))
```

```{r}
Prediction <- predict(fit, test, OOB=TRUE, type = "response")
cforestSolution <- data.frame(PassengerID = testPassengerId, Survived = Prediction)
write.csv(cforestSolution, file = 'sols/cforestSMOTE.csv', row.names = F, quote = F)
```

Puntuacion: 0.80383 mejor que los otros dos balanceos pero peor que sin balancear

### rpart

```{r}
fit <- rpart(Survived ~ ., data=train, method="class")

Prediction <- predict(fit, test, type = "class")
rpartSolution <- data.frame(PassengerID = testPassengerId, Survived = Prediction)
write.csv(rpartSolution, file = 'sols/rpartSMOTE.csv', row.names = F, quote = F)
```
 
Puntuacion: 0.77512 mejor que up peor que sub, peor que el original

### xgboost

```{r}
predictorsNames = names(train)[names(train) != 'Survived'] #names of columns after data conversion
dtrain<-xgb.DMatrix(data=data.matrix(train[,predictorsNames]),label=train$Survived)
dtest<-data.matrix(test[,predictorsNames])

param <- list(  objective           = "binary:logistic", 
                booster             = "gbtree",
                eta                 = 0.001, #0.3 #0.3
                max_depth           = 8, #10
                subsample           = 0.5, #0.7
                colsample_bytree    = 0.6 #0.6
)

nround  = 50 #use 3000 he comentado los parametros anteriores #200
set.seed(2045)
clf <- xgboost(param = param, dtrain, nrounds=nround, min_child_weight = 1, verbose=0)

pred <- predict(clf, dtest)
pred_final <- ifelse(pred>0.5,1,0)

xgbSolution <- data.frame(PassengerID = testPassengerId, Survived = pred_final)
write.csv(xgbSolution, file = 'sols/xgbSMOTE.csv', row.names = F, quote = F)
```

Puntuacion: 0.79426 mejor que con los otros dos balanceos pero peor que el original

#Clasificación con limpieza de ruido

```{r}
fullCleaned <- read.csv('data/fullSMOTEandIPF.csv')

nTestRows <- 418
nRows <- nrow(fullCleaned)

train <- fullCleaned[1 : (nRows - nTestRows), ]
test <- fullCleaned[(nRows - nTestRows + 1) : nRows,]
testPassengerId <- 892:1309
```

### cforest

```{r}
set.seed(415)
fit <- cforest(as.factor(Survived) ~ .,
                 data = train, 
                 controls=cforest_unbiased(ntree=2000, mtry=3, trace = TRUE))
```

```{r}
Prediction <- predict(fit, test, OOB=TRUE, type = "response")
cforestSolution <- data.frame(PassengerID = testPassengerId, Survived = Prediction)
write.csv(cforestSolution, file = 'sols/cforestSMOTEandIPF.csv', row.names = F, quote = F)
```

Puntuacion: 0.77512

### rpart

```{r}
fit <- rpart(Survived ~ ., data=train, method="class")

Prediction <- predict(fit, test, type = "class")
rpartSolution <- data.frame(PassengerID = testPassengerId, Survived = Prediction)
write.csv(rpartSolution, file = 'sols/rpartSMOTEandIPF.csv', row.names = F, quote = F)
```
 
Puntuacion: 0.77033

### xgboost

```{r}
predictorsNames = names(train)[names(train) != 'Survived'] #names of columns after data conversion
dtrain<-xgb.DMatrix(data=data.matrix(train[,predictorsNames]),label=train$Survived)
dtest<-data.matrix(test[,predictorsNames])

param <- list(  objective           = "binary:logistic", 
                booster             = "gbtree",
                eta                 = 0.001, #0.3 #0.3
                max_depth           = 8, #10
                subsample           = 0.5, #0.7
                colsample_bytree    = 0.6 #0.6
)

nround  = 50 #use 3000 he comentado los parametros anteriores #200
set.seed(2045)
clf <- xgboost(param = param, dtrain, nrounds=nround, min_child_weight = 1, verbose=0)

pred <- predict(clf, dtest)
pred_final <- ifelse(pred>0.5,1,0)

xgbSolution <- data.frame(PassengerID = testPassengerId, Survived = pred_final)
write.csv(xgbSolution, file = 'sols/xgbSMOTEandIPF.csv', row.names = F, quote = F)
```

Puntuacion: 0.77033

